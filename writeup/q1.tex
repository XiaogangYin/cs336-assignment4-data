\titledquestion{Filtering Common Crawl}[44] 
\begin{parts}
    \part[4] Problem (look\_at\_cc)
        \begin{subparts}
          \subpart Download the WARC file above, or find the copy we provide on the cluster. Let’s look at the
          first page in this file. This is a gzipped file, and you can browse its contents with:
          \begin{lstlisting}[language=Python]
          $ zcat /data/CC/example.warc.gz | less
          \end{lstlisting}
          less lets you browse the file using keyboard arrows, Page Up, Page Down. To exit, press “q”.
          Look at the very first web page. What is its URL? Is it still accessible? Can you tell what the
          page seems to be about by looking at the raw HTML?

          \textbf{Deliverable}: A 2-3 sentence response. 

          \ifans{I use gzcat command in my macOS(zcat reports an error). Its URL is \url{http://0371rykj.com/ipfhsb/34.html}. It is not accessible(It is not safe, and is blocked by my chrome. The page seems to be a small company site by looking at the raw HTML.)} 

          \subpart Let’s now look at the corresponding WET file:
            \begin{lstlisting}[language=Python]
            $ zcat /data/CC/example.warc.wet.gz | less
            \end{lstlisting}
            Note that the WET files contain HTTP headers (e.g., Content-Length) that are not part of the
            extracted text contents. If you look at the first example, you will see that it contains text that
            was extracted from the raw HTML you just saw.
            Notice that much of the extracted text is reminiscent of the HTML structure, and not actually
            the page’s main content. Are there parts of the text you see that you think should have been
            filtered out by the extractor? Think about the quality of this text as training data: what might
            go wrong in training a model on text that looks like this? Conversely, what useful information
            can a model potentially extract from this page?

            \textbf{Deliverable}: A 3-4 sentence response.

            \ifans{Yes, some parts of the text I see should have been filtered out. There are some porngraphic content. Some bad content  will be generated by the model which is trained on text that looks like this. In this page, the product introduction can be useful.}

          \subpart What makes a good training example is highly contextual. Describe an application domain for
            which this example might be useful to have in the training data, and one where it might not be.

            \textbf{Deliverable}: A 1-2 sentence response.

            \ifans{Useful domain: product }

          \subpart Let’s look at some more examples to get a better sense of what’s in the Common Crawl. Look
            through 25 more WET records. For each record, very briefly comment on the document’s language
            (if you can identify it), the domain name, what type of page it is, etc. How many examples does
            it take until you see what you’d deem a “high-quality” webpage?

            \textbf{Deliverable}: Brief annotations of 25 documents with the document’s language, domain, type of
            page, and any other miscellaneous notes about the document. The number of examples it takes
            until you see a high-quality example.

            \ifans{
            \begin{tabular}{llll}
              \toprule
              No. & language & domain & type \\
              \midrule
              1 & Traditional Chinese & product introduction & content page
               \\
              2 & Chinese & movie comment & forum list page
               \\
              3 & English & science & content page
               \\
              4 & Traditional Chinese & porn & content page
               \\
              5 & Traditional Chinese & product introduction & content page
               \\
              6 & Chinese & error page & none page
               \\
              7 & Traditional Chinese & porn & content page
               \\
              8 & Dutch & present & nothing page
               \\
              9 & Greek & education page & content page
               \\
              10 & Greek & forum page & list page
               \\
              11 & Chinese & casino page & content page
               \\
              12 & Turkish & computer tech page & list page(good)
               \\
              13 & English & casino page & content page(good)
               \\
              14 & English & none404 page & bad page
               \\
              15 & Chinese & casino page & content page(good)
               \\
              16 & Traditional Chinese & product introduction & content page(ok http://303323.com/xydt/dnqgqzl-65615.html)
               \\
              17 & Chinese & movie content page & content page
               \\
              18 & Chinese & hospital introduction page & conteng page(good)
               \\
              19 & Traditional Chinese & porn & content page
               \\
              20 & English & search page & none page
               \\
              21 & Traditional Chinese & porn & content page
               \\
              22 & Traditional Chinese & porn & content page
               \\
              23 & Traditional Chinese & porn & content page
               \\
              24 & Spanish & news page & content page(good)
               \\
              25 & Danish & service introduction & content page
               \\
              \bottomrule
              \end{tabular}
              it takes about 10 examples
            until I see a high-quality example.
            }
        \end{subparts}

\part[3] Problem (extract\_text)        
  \begin{subparts}
    \subpart Write a function that extracts text from a byte string containing raw HTML. Use
      \textit{resiliparse.extract.html2text.extract\_plain\_text} to perform the extraction. This function
      needs a string, so you will need to first decode the byte string into a Unicode string. Be
      aware that the input byte string might not be encoded in UTF-8, so your function should be able
      to detect the encoding in case UTF-8 fails. Resiliparse also offers
      \textit{resiliparse.parse.encoding.detect\_encoding()}, which might be useful.

      \textbf{Deliverable}: A function that takes a byte string containing HTML and returns a string containing
      the extracted text. Implement the adapter  \textcolor{red}{[run\_extract\_text\_from\_html\_bytes]} and
      make sure it passes \textit{uv run pytest -k test\_extract\_text\_from\_html\_bytes}

      \ifans{cs336\_data/extract\_text.py}

    \subpart Run your text extraction function on a single WARC file. Compare its output to the extracted
      text in the corresponding WET file. What differences and/or similarities do you notice? Which
      extraction seems better?

      \textbf{Deliverable}: 2-3 sentence response comparing and contrasting the text extracted by your own
      function versus the extracted text in the WET files.

      \ifans{Its output has more unwanted character and space. WET's output seems better.}

  \end{subparts}
\part[6] Problem (language\_identification)
  \begin{subparts}
    \subpart Write a function that will take a Unicode string and identify the main language that is present
      in this string. Your function should return a pair, containing an identifier of the language and a
      score between 0 and 1 representing its confidence in that prediction.

      \textbf{Deliverable}: A function that performs language identification, giving its top language prediction
      and a score. Implement the adapter \textcolor{red}{[run\_identify\_language]} and make sure it passes both
      tests in uv run pytest -k test\_identify\_language . Note that these tests assume a particular
      string identifier for English (“en”) and Chinese (“zh”), so your test adapter should perform any
      applicable re-mapping, if necessary.

      \ifans{cs336\_data/language\_identification.py}

    \subpart The behavior of language models at inference time largely depends on the data they were trained
      on. As a result, issues in the data filtering pipeline can result in problems downstream. What
      issues do you think could arise from problems in the language identification procedure? In a
      higher-stakes scenario (such as when deploying a user-facing product), how would you go about
      mitigating these issues?

      \textbf{Deliverable}: A 2-5 sentence response.

      \ifans{It will reduce the accuracy of downstream tasks (such as recognizing Chinese as Japanese). In a
      higher-stakes scenario (such as when deploying a user-facing product), we can raise the threshold or reject some requests or use more advanced models.}


    \subpart Run your language identification system on text extracted from the WARC files (via your
      previously-implemented text extraction function). Manually identify the language in 20 random
      examples and compare your labels with the classifier predictions. Report any classifier errors.
      What fraction of documents are English? Based on your observations, what would be a suitable
      classifier confidence threshold to use in filtering?
      
      \textbf{Deliverable}: A 2-5 sentence response.

      \ifans{
\begin{tabular}{lllll}
\toprule
id & model\_class & score & man\_result & right \\
\midrule
2313010a-88be-49af-862f-161877b9699e & el & 0.999869168 & Greek & 1
 \\
7fe201e8-f0ba-40b3-9dc2-0732347e913d & zh & 0.978632092 & Chinese & 1
 \\
eca3f0b3-064e-4c23-ab2d-2701baf06380 & ca & 0.954317212 & Catalan & 1
 \\
b9cab2be-07d2-4314-a38f-a04a3106fbd0 & tr & 0.988240182 & Turkish & 1
 \\
5c9cfa3a-0f6c-460a-a69e-7d5489b99bc0 & en & 0.110449299 & English & 1
 \\
988fe7b4-3dd0-45e2-8ffa-fd06569f75d4 & en & 0.518493354 & English & 1
 \\
0f7f7694-e93c-4b97-a119-5f9e3a534697 & zh & 0.872076213 & Chinese & 1
 \\
b43f8a93-7f32-4f22-8578-9f4a0d959447 & zh & 0.985808253 & Chinese & 1
 \\
2666d698-c60f-49bd-9361-58333a957065 & pt & 0.927117705 & Portugal & 1
 \\
3ea00414-38db-4ea6-a917-c15370e4099d & en & 0.604694903 & English & 1
 \\
e588aaa9-0c8f-4d81-9bf9-8e0cabc9b374 & sv & 0.815790653 & Swedish & 1
 \\
a1f13415-35ce-4f04-9833-43f6e462f09d & nl & 0.909454644 & Dutch & 1
 \\
08a56fb9-1104-49e8-969f-17d1e7795315 & en & 0.764572859 & English & 1
 \\
b7c3f90f-7448-4a48-a472-9a741f07eac1 & de & 0.913456917 & German & 1
 \\
dc5aabbb-c532-4f90-830d-c3b0a3fb471a & en & 0.30128774 & English & 1
 \\
ed505555-2a41-4eeb-9e59-845fd0c50d43 & en & 0.363320649 & Portugal & 0
 \\
643fedb7-4304-43cf-9ef4-4979f2d519f7 & ru & 0.933857203 & Russian & 1
 \\
0168ae62-e9af-4469-8ffa-1ac616184ce9 & es & 0.981798589 & Spanish & 1
 \\
0105d4a1-3390-410a-92e8-890d82744f8a & en & 0.715703964 & Chinese & 0
 \\
91f6840c-9efc-4c27-975b-dbc29a24f5e2 & en & 0.818909943 & English & 1
 \\
\bottomrule
\end{tabular}

        ed505555-2a41-4eeb-9e59-845fd0c50d43 and 0105d4a1-3390-410a-92e8-890d82744f8a are errors. 8/20 are English. Based on my observations, 0.5 would be a suitable
        classifier confidence threshold to use for languages ​​other than English. For English, the situation is more complex. English often appears alongside other languages, so we need to use the top 2 score gap value to filter.    
      }



      
  \end{subparts}

          
\end{parts}
