\titledquestion{Filtering Common Crawl}[44] 
\begin{parts}
    \part[4] Problem (look\_at\_cc)
        \begin{subparts}
          \subpart Download the WARC file above, or find the copy we provide on the cluster. Let’s look at the
          first page in this file. This is a gzipped file, and you can browse its contents with:
          \begin{lstlisting}[language=Python]
          $ zcat /data/CC/example.warc.gz | less
          \end{lstlisting}
          less lets you browse the file using keyboard arrows, Page Up, Page Down. To exit, press “q”.
          Look at the very first web page. What is its URL? Is it still accessible? Can you tell what the
          page seems to be about by looking at the raw HTML?

          \textbf{Deliverable}: A 2-3 sentence response. 

          \ifans{I use gzcat command in my macOS(zcat reports an error). Its URL is \url{http://0371rykj.com/ipfhsb/34.html}. It is not accessible(It is not safe, and is blocked by my chrome. The page seems to be a small company site by looking at the raw HTML.)} 

          \subpart Let’s now look at the corresponding WET file:
            \begin{lstlisting}[language=Python]
            $ zcat /data/CC/example.warc.wet.gz | less
            \end{lstlisting}
            Note that the WET files contain HTTP headers (e.g., Content-Length) that are not part of the
            extracted text contents. If you look at the first example, you will see that it contains text that
            was extracted from the raw HTML you just saw.
            Notice that much of the extracted text is reminiscent of the HTML structure, and not actually
            the page’s main content. Are there parts of the text you see that you think should have been
            filtered out by the extractor? Think about the quality of this text as training data: what might
            go wrong in training a model on text that looks like this? Conversely, what useful information
            can a model potentially extract from this page?

            \textbf{Deliverable}: A 3-4 sentence response.

            \ifans{Yes, some parts of the text I see should have been filtered out. There are some porngraphic content. Some bad content  will be generated by the model which is trained on text that looks like this. In this page, the product introduction can be useful.}

          \subpart What makes a good training example is highly contextual. Describe an application domain for
            which this example might be useful to have in the training data, and one where it might not be.

            \textbf{Deliverable}: A 1-2 sentence response.

            \ifans{Useful domain: product }

          \subpart Let’s look at some more examples to get a better sense of what’s in the Common Crawl. Look
            through 25 more WET records. For each record, very briefly comment on the document’s language
            (if you can identify it), the domain name, what type of page it is, etc. How many examples does
            it take until you see what you’d deem a “high-quality” webpage?

            \textbf{Deliverable}: Brief annotations of 25 documents with the document’s language, domain, type of
            page, and any other miscellaneous notes about the document. The number of examples it takes
            until you see a high-quality example.

            \ifans{
            \begin{tabular}{llll}
              \toprule
              No. & language & domain & type \\
              \midrule
              1 & Traditional Chinese & product introduction & content page
               \\
              2 & Chinese & movie comment & forum list page
               \\
              3 & English & science & content page
               \\
              4 & Traditional Chinese & porn & content page
               \\
              5 & Traditional Chinese & product introduction & content page
               \\
              6 & Chinese & error page & none page
               \\
              7 & Traditional Chinese & porn & content page
               \\
              8 & Dutch & present & nothing page
               \\
              9 & Greek & education page & content page
               \\
              10 & Greek & forum page & list page
               \\
              11 & Chinese & casino page & content page
               \\
              12 & Turkish & computer tech page & list page(good)
               \\
              13 & English & casino page & content page(good)
               \\
              14 & English & none404 page & bad page
               \\
              15 & Chinese & casino page & content page(good)
               \\
              16 & Traditional Chinese & product introduction & content page(ok http://303323.com/xydt/dnqgqzl-65615.html)
               \\
              17 & Chinese & movie content page & content page
               \\
              18 & Chinese & hospital introduction page & conteng page(good)
               \\
              19 & Traditional Chinese & porn & content page
               \\
              20 & English & search page & none page
               \\
              21 & Traditional Chinese & porn & content page
               \\
              22 & Traditional Chinese & porn & content page
               \\
              23 & Traditional Chinese & porn & content page
               \\
              24 & Spanish & news page & content page(good)
               \\
              25 & Danish & service introduction & content page
               \\
              \bottomrule
              \end{tabular}
              it takes about 10 examples
            until I see a high-quality example.
            }
        \end{subparts}

\part[3] Problem (extract\_text)        
  \begin{subparts}
    \subpart Write a function that extracts text from a byte string containing raw HTML. Use
      \textit{resiliparse.extract.html2text.extract\_plain\_text} to perform the extraction. This function
      needs a string, so you will need to first decode the byte string into a Unicode string. Be
      aware that the input byte string might not be encoded in UTF-8, so your function should be able
      to detect the encoding in case UTF-8 fails. Resiliparse also offers
      \textit{resiliparse.parse.encoding.detect\_encoding()}, which might be useful.

      \textbf{Deliverable}: A function that takes a byte string containing HTML and returns a string containing
      the extracted text. Implement the adapter  \textcolor{red}{[run\_extract\_text\_from\_html\_bytes]} and
      make sure it passes \textit{uv run pytest -k test\_extract\_text\_from\_html\_bytes}

      \ifans{cs336\_data/extract\_text.py}

    \subpart Run your text extraction function on a single WARC file. Compare its output to the extracted
      text in the corresponding WET file. What differences and/or similarities do you notice? Which
      extraction seems better?

      \textbf{Deliverable}: 2-3 sentence response comparing and contrasting the text extracted by your own
      function versus the extracted text in the WET files.
  \end{subparts}
            

\end{parts}
